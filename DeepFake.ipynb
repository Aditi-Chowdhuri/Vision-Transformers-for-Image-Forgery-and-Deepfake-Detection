{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y dlib\n",
        "# !sudo apt-get update -qq\n",
        "# !sudo apt-get install -y -qq build-essential cmake pkg-config python3-dev \\\n",
        "#   libopenblas-dev liblapack-dev\n",
        "\n",
        "# import os, sys\n",
        "\n",
        "# os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "# os.environ[\"CUDACXX\"] = \"/usr/local/cuda/bin/nvcc\"\n",
        "# os.environ[\"FORCE_CMAKE\"] = \"1\"\n",
        "\n",
        "# # Key: 75-real => SASS only for Tesla T4, avoids PTX JIT (the thing throwing code 222)\n",
        "# os.environ[\"CMAKE_ARGS\"] = \" \".join([\n",
        "#     \"-DDLIB_USE_CUDA=1\",\n",
        "#     \"-DDLIB_NO_GUI_SUPPORT=1\",\n",
        "#     f\"-DPYTHON_EXECUTABLE={sys.executable}\",\n",
        "#     \"-DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\",\n",
        "#     \"-DCMAKE_BUILD_TYPE=Release\",\n",
        "#     \"-DCMAKE_CUDA_ARCHITECTURES=75-real\",\n",
        "# ])\n",
        "\n",
        "\n",
        "# !pip install -v --no-cache-dir --no-build-isolation --no-binary :all: dlib==19.24.6\n",
        "\n",
        "# import dlib\n",
        "# print(\"dlib:\", dlib.__version__)\n",
        "# print(\"DLIB_USE_CUDA:\", getattr(dlib, \"DLIB_USE_CUDA\", None))\n"
      ],
      "metadata": {
        "id": "EMi83KjF9hal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZpEqZ7P5OZY",
        "outputId": "4f23389d-b8da-46b9-f5ad-649bbdf94a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, roc_auc_score, confusion_matrix,\n",
        "    roc_curve, auc, precision_score, recall_score\n",
        ")\n",
        "\n",
        "import dlib\n"
      ],
      "metadata": {
        "id": "3STZ3JVpALSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = '/content/drive/MyDrive/ff_c23_data'\n",
        "REAL_PATH = os.path.join(ROOT_PATH, \"original\")\n",
        "FAKE_PATH = os.path.join(ROOT_PATH, \"Deepfakes\")\n",
        "OUTPUT_FRAME_SIZE = (224, 224)\n",
        "FRAME_COUNT = 30\n",
        "MAX_VIDEOS = 500\n",
        "NUM_WORKERS = 2\n",
        "BATCH_SIZE = 8\n",
        "PADDING_FACTOR = 1.3\n",
        "LR_FINE_TUNE = 1e-6 # Learning Rate ban đầu cho Fine-Tuning\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS_PHASE1 = 20\n",
        "EPOCHS_PHASE2 = 60\n",
        "LR_PHASE1 = 1e-4\n",
        "LR_PHASE2 = 1e-6\n",
        "\n",
        "PADDING_FACTOR = 1.3\n",
        "EARLY_STOP_PATIENCE = 10\n",
        "\n",
        "# Model options\n",
        "BACKBONE_NAME = \"xception\"   # from timm\n",
        "HIDDEN_SIZE = 128\n",
        "DROPOUT = 0.4\n",
        "USE_SPATIAL_FLATTEN = False  # True = match original (huge vector), False = GAP (recommended)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cokmNXp25Slc",
        "outputId": "2b75d1e4-315e-4072-a6e1-6f439030b20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Frame extraction with dlib face cropping (same logic as original)\n",
        "# --------------------\n",
        "face_detector_dlib = dlib.get_frontal_face_detector()\n",
        "\n",
        "def extract_frames(video_path, output_size=OUTPUT_FRAME_SIZE, frame_count=FRAME_COUNT, padding_factor=PADDING_FACTOR):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if total_frames == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    step = max(total_frames // frame_count, 1)\n",
        "\n",
        "    for i in range(frame_count):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector_dlib(gray)\n",
        "\n",
        "        frame_h, frame_w, _ = frame.shape\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            d = faces[0]\n",
        "\n",
        "            y_center = (d.top() + d.bottom()) // 2\n",
        "            x_center = (d.left() + d.right()) // 2\n",
        "            h = d.bottom() - d.top()\n",
        "            w = d.right() - d.left()\n",
        "\n",
        "            h_pad = int(h * padding_factor)\n",
        "            w_pad = int(w * padding_factor)\n",
        "\n",
        "            y1 = max(0, y_center - h_pad // 2)\n",
        "            y2 = min(frame_h, y_center + h_pad // 2)\n",
        "            x1 = max(0, x_center - w_pad // 2)\n",
        "            x2 = min(frame_w, x_center + w_pad // 2)\n",
        "\n",
        "            cropped_face = frame[y1:y2, x1:x2]\n",
        "\n",
        "            if cropped_face.size != 0:\n",
        "                resized_frame = cv2.resize(cropped_face, output_size)\n",
        "                frames.append(resized_frame)\n",
        "                continue\n",
        "\n",
        "        resized_frame = cv2.resize(frame, output_size)\n",
        "        frames.append(resized_frame)\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames) if len(frames) == frame_count else np.array([])\n",
        "\n",
        "# !wget -q http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
        "# !bzip2 -dk mmod_human_face_detector.dat.bz2  # produces mmod_human_face_detector.dat\n",
        "# !ls -lh mmod_human_face_detector.dat*\n",
        "\n",
        "# CNN_MODEL_PATH = \"mmod_human_face_detector.dat\"\n",
        "# cnn_face_detector = dlib.cnn_face_detection_model_v1(CNN_MODEL_PATH)\n",
        "\n",
        "# def extract_frames(video_path, output_size=OUTPUT_FRAME_SIZE, frame_count=FRAME_COUNT, padding_factor=PADDING_FACTOR):\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "#     frames = []\n",
        "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "#     if total_frames == 0:\n",
        "#         cap.release()\n",
        "#         return np.array([])\n",
        "\n",
        "#     step = max(total_frames // frame_count, 1)\n",
        "\n",
        "#     for i in range(frame_count):\n",
        "#         cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "#         ret, frame = cap.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         # CNN detector expects RGB\n",
        "#         rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#         detections = cnn_face_detector(rgb, 1)  # upsample=1\n",
        "\n",
        "#         frame_h, frame_w, _ = frame.shape\n",
        "\n",
        "#         if len(detections) > 0:\n",
        "#             # pick the first detection (same “take first face” logic as before)\n",
        "#             rect = detections[0].rect  # mmod_rectangle -> .rect is a dlib.rectangle\n",
        "\n",
        "#             top = rect.top()\n",
        "#             bottom = rect.bottom()\n",
        "#             left = rect.left()\n",
        "#             right = rect.right()\n",
        "\n",
        "#             y_center = (top + bottom) // 2\n",
        "#             x_center = (left + right) // 2\n",
        "#             h = bottom - top\n",
        "#             w = right - left\n",
        "\n",
        "#             h_pad = int(h * padding_factor)\n",
        "#             w_pad = int(w * padding_factor)\n",
        "\n",
        "#             y1 = max(0, y_center - h_pad // 2)\n",
        "#             y2 = min(frame_h, y_center + h_pad // 2)\n",
        "#             x1 = max(0, x_center - w_pad // 2)\n",
        "#             x2 = min(frame_w, x_center + w_pad // 2)\n",
        "\n",
        "#             cropped_face = frame[y1:y2, x1:x2]\n",
        "\n",
        "#             if cropped_face.size != 0:\n",
        "#                 resized_frame = cv2.resize(cropped_face, output_size)\n",
        "#                 frames.append(resized_frame)\n",
        "#                 continue\n",
        "\n",
        "#         resized_frame = cv2.resize(frame, output_size)\n",
        "#         frames.append(resized_frame)\n",
        "\n",
        "#     cap.release()\n",
        "#     return np.array(frames) if len(frames) == frame_count else np.array([])\n"
      ],
      "metadata": {
        "id": "YW0f-tm_5voh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Load data into memory (same split: 70/15/15)\n",
        "# --------------------\n",
        "data, labels = [], []\n",
        "\n",
        "print(f\"Loading up to {MAX_VIDEOS} REAL videos...\")\n",
        "for video_file in tqdm(os.listdir(REAL_PATH)[:MAX_VIDEOS]):\n",
        "    frames = extract_frames(os.path.join(REAL_PATH, video_file))\n",
        "    if frames.size != 0:\n",
        "        data.append(frames)\n",
        "        labels.append(0)\n",
        "\n",
        "print(f\"Loading up to {MAX_VIDEOS} FAKE videos...\")\n",
        "for video_file in tqdm(os.listdir(FAKE_PATH)[:MAX_VIDEOS]):\n",
        "    frames = extract_frames(os.path.join(FAKE_PATH, video_file))\n",
        "    if frames.size != 0:\n",
        "        data.append(frames)\n",
        "        labels.append(1)\n",
        "\n",
        "print(\"Total loaded videos:\", len(data))\n",
        "\n",
        "data = np.array(data)     # (N, T, H, W, C) uint8\n",
        "labels = np.array(labels) # (N,)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    data, labels, test_size=0.3, random_state=SEED, stratify=labels\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Train/Val/Test:\", X_train.shape, X_val.shape, X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynbVc2zj7ThW",
        "outputId": "b41613de-fcd7-43ee-9a73-4095fdfa0b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading up to 500 REAL videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [2:34:53<00:00, 18.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading up to 500 FAKE videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 160/500 [43:14<1:38:34, 17.40s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(X_train, ROOT_PATH + '/X_train_full.pt')\n",
        "torch.save(y_train, ROOT_PATH + '/y_train_full.pt')\n",
        "torch.save(X_val, ROOT_PATH + '/X_val_full.pt')\n",
        "torch.save(y_val, ROOT_PATH + '/y_val_full.pt')\n",
        "torch.save(X_test, ROOT_PATH + '/X_test_full.pt')\n",
        "torch.save(y_test, ROOT_PATH + '/y_test_full.pt')"
      ],
      "metadata": {
        "id": "QR-PSd6rKaHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# PyTorch Dataset / DataLoader\n",
        "# --------------------\n",
        "from torchvision import transforms\n",
        "\n",
        "# ImageNet normalization (timm's xception expects this style)\n",
        "imagenet_mean = (0.485, 0.456, 0.406)\n",
        "imagenet_std  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_frame_tfms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),  # uint8 -> float in [0,1]\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "val_frame_tfms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "class VideoNumpyDataset(Dataset):\n",
        "    def __init__(self, X, y, frame_transform):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.frame_transform = frame_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.X[idx]  # (T,H,W,C) uint8\n",
        "        label = int(self.y[idx])\n",
        "\n",
        "        frames = []\n",
        "        for t in range(video.shape[0]):\n",
        "            frame = video[t]  # (H,W,C) BGR from cv2\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(self.frame_transform(frame))  # (C,H,W)\n",
        "\n",
        "        video_tensor = torch.stack(frames, dim=0)  # (T,C,H,W)\n",
        "        return video_tensor, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "train_ds = VideoNumpyDataset(X_train, y_train, train_frame_tfms)\n",
        "val_ds   = VideoNumpyDataset(X_val,   y_val,   val_frame_tfms)\n",
        "test_ds  = VideoNumpyDataset(X_test,  y_test,  val_frame_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
        "\n",
        "# Class weights (for imbalance)\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "print(\"class_weights:\", class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW6Uxbb79O_C",
        "outputId": "da467626-a0ba-447c-f7ad-d0f4a43ab55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_weights: [1.00359712 0.99642857]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7a1Q4aS9Rlr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}